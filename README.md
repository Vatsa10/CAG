# CAG
Implemention of Cache Augmented Generation using Phi4

In essence, RAG′s approach is to retain external knowledge and encode it into vectors that act as key points stored in a vector database. The input query is also transformed into a vector prior to querying the LLM, and the knowledge vectors with the most similarity to the query vector are pulled. The retrieved information is usually appended to the prompt used to query the LLM to produce a response. This method is powerful, and theoretically scale to very large sources of knowledge. It adds a possible error on document selection that depends on document chunking and on the embedding model used to populate the vector database.

CAG takes the simpler solution. If your external knowledge base is small enough then CAG consists of simply including the entire knowledge base in the prompt together with the query. The LLM can then take in both the query and the knowledge base and generate a response. This approach does not require a vector database or similarity calculations. Recent innovations in LLMs (like Llama, Mixtral, Gemma, etc.) make CAG work. These models show much better performance and efficiency in longer context windows.

Well, a naive implementation, where you stick the entire knowledge base into every single prompt for CAG and ask it for a causal explanation, results in very slow inference times. This is because LLMs generate one token at a time, and each generation relies on the whole preceding context. And here’s where the major innovation of CAG happens: by preloading the knowledge base into the model’s context, and using a dynamic caching strategy (namely, Key-Value caching), allow the model to cache and reuse the knowledge base instead of processing it over and over for every new query. It is important to highlight that the model actually “remembers” the knowledge that it was processed during the training phase, thus is able to pay attention to only the query during the inference phase.
